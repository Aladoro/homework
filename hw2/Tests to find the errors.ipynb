{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import logz\n",
    "import scipy.signal\n",
    "import os\n",
    "import time\n",
    "import inspect\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(\n",
    "\t\tinput_placeholder, \n",
    "\t\toutput_size,\n",
    "\t\tscope, \n",
    "\t\tn_layers=2, \n",
    "\t\tsize=64, \n",
    "\t\tactivation=tf.tanh,\n",
    "\t\toutput_activation=None\n",
    "\t\t):\n",
    "\t#========================================================================================#\n",
    "\t#                           ----------SECTION 3----------\n",
    "\t# Network building\n",
    "\t#\n",
    "\t# Your code should make a feedforward neural network (also called a multilayer perceptron)\n",
    "\t# with 'n_layers' hidden layers of size 'size' units. \n",
    "\t# \n",
    "\t# The output layer should have size 'output_size' and activation 'output_activation'.\n",
    "\t#\n",
    "\t# Hint: use tf.layers.dense\n",
    "\t#========================================================================================#\n",
    "\n",
    "\twith tf.variable_scope(scope):\n",
    "\t\t# YOUR_CODE_HERE\n",
    "\t\tout = tf.layers.dense(input_placeholder, size, activation=activation, name=\"fcfirst\")\n",
    "\t\tfor i in range(n_layers - 2):\n",
    "\t\t\tout = tf.layers.dense(out, size, activation=activation, name = \"fc\" + str(i+1))\n",
    "\t\tout = tf.layers.dense(out, output_size, activation=output_activation, name = \"fclast\")\n",
    "\t\treturn out\n",
    "def pathlength(path):\n",
    "\treturn len(path[\"reward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-03 23:58:57,185] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n",
      "********** Iteration 0 ************\n",
      "----------------------------------------\n",
      "|               Time |           0.234 |\n",
      "|          Iteration |               0 |\n",
      "|      AverageReturn |            23.3 |\n",
      "|          StdReturn |            11.4 |\n",
      "|          MaxReturn |              59 |\n",
      "|          MinReturn |              10 |\n",
      "|          EpLenMean |            23.3 |\n",
      "|           EpLenStd |            11.4 |\n",
      "| TimestepsThisBatch |           1e+03 |\n",
      "|     TimestepsSoFar |           1e+03 |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "join() argument must be str or bytes, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0b10830dcdc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[0mlogz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_tabular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TimestepsSoFar\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[0mlogz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump_tabular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m     \u001b[0mlogz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpickle_tf_vars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\ML-DODI\\CS294-112-HW\\hw2\\logz.py\u001b[0m in \u001b[0;36mpickle_tf_vars\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m     81\u001b[0m     \u001b[0m_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"vars.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mujoco-env\\lib\\ntpath.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult_drive\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mresult_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBytesWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         \u001b[0mgenericpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_arg_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'join'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mujoco-env\\lib\\genericpath.py\u001b[0m in \u001b[0;36m_check_arg_types\u001b[1;34m(funcname, *args)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             raise TypeError('%s() argument must be str or bytes, not %r' %\n\u001b[1;32m--> 143\u001b[1;33m                             (funcname, s.__class__.__name__)) from None\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasstr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasbytes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can't mix strings and bytes in path components\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: join() argument must be str or bytes, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "exp_name=''\n",
    "env_name='CartPole-v0'\n",
    "n_iter=100 \n",
    "gamma=1.0\n",
    "min_timesteps_per_batch=1000\n",
    "max_path_length=None\n",
    "learning_rate=5e-3 \n",
    "reward_to_go=True\n",
    "animate=False\n",
    "logdir=None \n",
    "normalize_advantages=True\n",
    "nn_baseline=False\n",
    "seed=0\n",
    "# network arguments\n",
    "n_layers=1\n",
    "size=32\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Configure output directory for logging\n",
    "\"\"\"\n",
    "logz.configure_output_dir(logdir)\n",
    "\n",
    "# Log experimental parameters\n",
    "args = inspect.getargspec(train_PG)[0]\n",
    "locals_ = locals()\n",
    "params = {k: locals_[k] if k in locals_ else None for k in args}\n",
    "logz.save_params(params)\n",
    "\n",
    "# Set random seeds\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\"\"\"\n",
    "# Make the gym environment\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Is this env continuous, or discrete?\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "# Maximum length for episodes\n",
    "max_path_length = max_path_length or env.spec.max_episode_steps\n",
    "\n",
    "#========================================================================================#\n",
    "# Notes on notation:\n",
    "# \n",
    "# Symbolic variables have the prefix sy_, to distinguish them from the numerical values\n",
    "# that are computed later in the function\n",
    "# \n",
    "# Prefixes and suffixes:\n",
    "# ob - observation \n",
    "# ac - action\n",
    "# _no - this tensor should have shape (batch size /n/, observation dim)\n",
    "# _na - this tensor should have shape (batch size /n/, action dim)\n",
    "# _n  - this tensor should have shape (batch size /n/)\n",
    "# \n",
    "# Note: batch size /n/ is defined at runtime, and until then, the shape for that axis\n",
    "# is None\n",
    "#========================================================================================#\n",
    "\n",
    "# Observation and action sizes\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "#========================================================================================#\n",
    "#                           ----------SECTION 4----------\n",
    "# Placeholders\n",
    "# \n",
    "# Need these for batch observations / actions / advantages in policy gradient loss function.\n",
    "#========================================================================================#\n",
    "\n",
    "sy_ob_no = tf.placeholder(shape=[None, ob_dim], name=\"ob\", dtype=tf.float32)\n",
    "if discrete:\n",
    "    sy_ac_na = tf.placeholder(shape=[None], name=\"ac\", dtype=tf.int32) \n",
    "else:\n",
    "    sy_ac_na = tf.placeholder(shape=[None, ac_dim], name=\"ac\", dtype=tf.float32) \n",
    "\n",
    "# Define a placeholder for advantages\n",
    "sy_adv_n = tf.placeholder(shape=[None], name=\"adv\", dtype=tf.float32)\n",
    "\n",
    "\n",
    "#========================================================================================#\n",
    "#                           ----------SECTION 4----------\n",
    "# Networks\n",
    "# \n",
    "# Make symbolic operations for\n",
    "#   1. Policy network outputs which describe the policy distribution.\n",
    "#       a. For the discrete case, just logits for each action.\n",
    "#\n",
    "#       b. For the continuous case, the mean / log std of a Gaussian distribution over \n",
    "#          actions.\n",
    "#\n",
    "#      Hint: use the 'build_mlp' function you defined in utilities.\n",
    "#\n",
    "#      Note: these ops should be functions of the placeholder 'sy_ob_no'\n",
    "#\n",
    "#   2. Producing samples stochastically from the policy distribution.\n",
    "#       a. For the discrete case, an op that takes in logits and produces actions.\n",
    "#\n",
    "#          Should have shape [None]\n",
    "#\n",
    "#       b. For the continuous case, use the reparameterization trick:\n",
    "#          The output from a Gaussian distribution with mean 'mu' and std 'sigma' is\n",
    "#\n",
    "#               mu + sigma * z,         z ~ N(0, I)\n",
    "#\n",
    "#          This reduces the problem to just sampling z. (Hint: use tf.random_normal!)\n",
    "#\n",
    "#          Should have shape [None, ac_dim]\n",
    "#\n",
    "#      Note: these ops should be functions of the policy network output ops.\n",
    "#\n",
    "#   3. Computing the log probability of a set of actions that were actually taken, \n",
    "#      according to the policy.\n",
    "#\n",
    "#      Note: these ops should be functions of the placeholder 'sy_ac_na', and the \n",
    "#      policy network output ops.\n",
    "#   \n",
    "#========================================================================================#\n",
    "\n",
    "if discrete:\n",
    "    # YOUR_CODE_HERE\n",
    "    sy_logits_na = build_mlp(sy_ob_no, ac_dim, \"nn\", size=size, n_layers=n_layers)\n",
    "    print(tf.shape(sy_logits_na))\n",
    "    sy_sampled_ac = tf.reshape(tf.multinomial(tf.nn.log_softmax(sy_logits_na), tf.shape(sy_ob_no)[0]), [tf.shape(sy_ob_no)[0]]) # Hint: Use the tf.multinomial op\n",
    "    sy_logprob_n = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(sy_ac_na, ac_dim), logits=sy_logits_na)\n",
    "\n",
    "else:\n",
    "    # YOUR_CODE_HERE\n",
    "    sy_mean = build_mlp(sy_ob_no, ac_dim, \"mean\", size=size, n_layers=n_layers)\n",
    "    sy_logstd = tf.log(tf.get_variable(name=\"stdev\", shape=[ac_dim, ac_dim])) # logstd should just be a trainable variable, not a network output.\n",
    "    sy_sampled_ac = tf.matmul(tf.random_normal([tf.shape(sy_ob_no)[0], ac_dim]), sy_logstd) + sy_mean\n",
    "    sy_logprob_n = tf.nn.softmax_cross_entropy_with_logits(labels=sy_ac_na, logits=sy_sampled_ac)  # Hint: Use the log probability under a multivariate gaussian. //still incorrect, todo\n",
    "\n",
    "\n",
    "\n",
    "#========================================================================================#\n",
    "#                           ----------SECTION 4----------\n",
    "# Loss Function and Training Operation\n",
    "#========================================================================================#\n",
    "\n",
    "loss = -1*tf.reduce_mean(sy_logprob_n*sy_adv_n) # Loss function that we'll differentiate to get the policy gradient.\n",
    "update_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "#========================================================================================#\n",
    "#                           ----------SECTION 5----------\n",
    "# Optional Baseline\n",
    "#========================================================================================#\n",
    "\n",
    "if nn_baseline:\n",
    "    baseline_prediction = tf.squeeze(build_mlp(\n",
    "                            sy_ob_no, \n",
    "                            1, \n",
    "                            \"nn_baseline\",\n",
    "                            n_layers=n_layers,\n",
    "                            size=size))\n",
    "    # Define placeholders for targets, a loss function and an update op for fitting a \n",
    "    # neural network baseline. These will be used to fit the neural network baseline. \n",
    "    # YOUR_CODE_HERE\n",
    "    baseline_update_op = TODO\n",
    "\n",
    "\n",
    "#========================================================================================#\n",
    "# Tensorflow Engineering: Config, Session, Variable initialization\n",
    "#========================================================================================#\n",
    "\n",
    "tf_config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1) \n",
    "\n",
    "sess = tf.Session(config=tf_config)\n",
    "sess.__enter__() # equivalent to `with sess:`\n",
    "tf.global_variables_initializer().run() #pylint: disable=E1101\n",
    "\n",
    "\n",
    "\n",
    "#========================================================================================#\n",
    "# Training Loop\n",
    "#========================================================================================#\n",
    "\n",
    "total_timesteps = 0\n",
    "\n",
    "for itr in range(n_iter):\n",
    "    print(\"********** Iteration %i ************\"%itr)\n",
    "\n",
    "    # Collect paths until we have enough timesteps\n",
    "    timesteps_this_batch = 0\n",
    "    paths = []\n",
    "    while True:\n",
    "        ob = env.reset()\n",
    "        obs, acs, rewards = [], [], []\n",
    "        animate_this_episode=(len(paths)==0 and (itr % 10 == 0) and animate)\n",
    "        steps = 0\n",
    "        while True:\n",
    "            if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "            obs.append(ob)\n",
    "            ac = sess.run(sy_sampled_ac, feed_dict={sy_ob_no : ob[None]})\n",
    "            ac = ac[0]\n",
    "            acs.append(ac)\n",
    "            ob, rew, done, _ = env.step(ac)\n",
    "            rewards.append(rew)\n",
    "            steps += 1\n",
    "            if done or steps > max_path_length:\n",
    "                break\n",
    "        path = {\"observation\" : np.array(obs), \n",
    "                \"reward\" : np.array(rewards), \n",
    "                \"action\" : np.array(acs)}\n",
    "        paths.append(path)\n",
    "        timesteps_this_batch += pathlength(path)\n",
    "        if timesteps_this_batch > min_timesteps_per_batch:\n",
    "            break\n",
    "    total_timesteps += timesteps_this_batch\n",
    "\n",
    "    # Build arrays for observation, action for the policy gradient update by concatenating \n",
    "    # across paths\n",
    "    ob_no = np.concatenate([path[\"observation\"] for path in paths])\n",
    "    ac_na = np.concatenate([path[\"action\"] for path in paths])\n",
    "\n",
    "    #====================================================================================#\n",
    "    #                           ----------SECTION 4----------\n",
    "    # Computing Q-values\n",
    "    #\n",
    "    # Your code should construct numpy arrays for Q-values which will be used to compute\n",
    "    # advantages (which will in turn be fed to the placeholder you defined above). \n",
    "    #\n",
    "    # Recall that the expression for the policy gradient PG is\n",
    "    #\n",
    "    #       PG = E_{tau} [sum_{t=0}^T grad log pi(a_t|s_t) * (Q_t - b_t )]\n",
    "    #\n",
    "    # where \n",
    "    #\n",
    "    #       tau=(s_0, a_0, ...) is a trajectory,\n",
    "    #       Q_t is the Q-value at time t, Q^{pi}(s_t, a_t),\n",
    "    #       and b_t is a baseline which may depend on s_t. \n",
    "    #\n",
    "    # You will write code for two cases, controlled by the flag 'reward_to_go':\n",
    "    #\n",
    "    #   Case 1: trajectory-based PG \n",
    "    #\n",
    "    #       (reward_to_go = False)\n",
    "    #\n",
    "    #       Instead of Q^{pi}(s_t, a_t), we use the total discounted reward summed over \n",
    "    #       entire trajectory (regardless of which time step the Q-value should be for). \n",
    "    #\n",
    "    #       For this case, the policy gradient estimator is\n",
    "    #\n",
    "    #           E_{tau} [sum_{t=0}^T grad log pi(a_t|s_t) * Ret(tau)]\n",
    "    #\n",
    "    #v       where\n",
    "    #\n",
    "    #           Ret(tau) = sum_{t'=0}^T gamma^t' r_{t'}.\n",
    "    #\n",
    "    #       Thus, you should compute\n",
    "    #\n",
    "    #           Q_t = Ret(tau)\n",
    "    #\n",
    "    #   Case 2: reward-to-go PG \n",
    "    #\n",
    "    #       (reward_to_go = True)\n",
    "    #\n",
    "    #       Here, you estimate Q^{pi}(s_t, a_t) by the discounted sum of rewards starting\n",
    "    #       from time step t. Thus, you should compute\n",
    "    #\n",
    "    #           Q_t = sum_{t'=t}^T gamma^(t'-t) * r_{t'}\n",
    "    #\n",
    "    #\n",
    "    # Store the Q-values for all timesteps and all trajectories in a variable 'q_n',\n",
    "    # like the 'ob_no' and 'ac_na' above. \n",
    "    #\n",
    "    #====================================================================================#\n",
    "\n",
    "    # YOUR_CODE_HERE\n",
    "    qs = []\n",
    "    if reward_to_go:\n",
    "        for l in range(len(paths)):\n",
    "            rewards = paths[l]['reward']\n",
    "            n_actions = rewards.shape[0]\n",
    "            q = np.zeros((n_actions))\n",
    "            for i in range(n_actions):\n",
    "                for j in range(n_actions-i):\n",
    "                    q[i] += rewards[i + j]*np.power(gamma, j)\n",
    "            qs.append(q)\n",
    "    else:\n",
    "        for l in range(len(paths)):\n",
    "            rewards = paths[l]['reward']\n",
    "            n_actions = rewards.shape[0]\n",
    "            q = np.zeros((n_actions))\n",
    "            for i in range(n_actions):\n",
    "                q += rewards[i]*np.power(gamma, i)\n",
    "            qs.append(q)\n",
    "    q_n = np.concatenate(qs)\n",
    "    #====================================================================================#\n",
    "    #                           ----------SECTION 5----------\n",
    "    # Computing Baselines\n",
    "    #====================================================================================#\n",
    "\n",
    "    if nn_baseline:\n",
    "        # If nn_baseline is True, use your neural network to predict reward-to-go\n",
    "        # at each timestep for each trajectory, and save the result in a variable 'b_n'\n",
    "        # like 'ob_no', 'ac_na', and 'q_n'.\n",
    "        #\n",
    "        # Hint #bl1: rescale the output from the nn_baseline to match the statistics\n",
    "        # (mean and std) of the current or previous batch of Q-values. (Goes with Hint\n",
    "        # #bl2 below.)\n",
    "\n",
    "        b_n = TODO\n",
    "        adv_n = q_n - b_n\n",
    "    else:\n",
    "        adv_n = q_n.copy()\n",
    "\n",
    "    #====================================================================================#\n",
    "    #                           ----------SECTION 4----------\n",
    "    # Advantage Normalization\n",
    "    #====================================================================================#\n",
    "\n",
    "    if normalize_advantages:\n",
    "        # On the next line, implement a trick which is known empirically to reduce variance\n",
    "        # in policy gradient methods: normalize adv_n to have mean zero and std=1. \n",
    "        # YOUR_CODE_HERE\n",
    "        mean = np.mean(adv_n)\n",
    "        stdev = np.std(adv_n)\n",
    "        adv_n = adv_n/stdev - mean\n",
    "\n",
    "\n",
    "    #====================================================================================#\n",
    "    #                           ----------SECTION 5----------\n",
    "    # Optimizing Neural Network Baseline\n",
    "    #====================================================================================#\n",
    "    if nn_baseline:\n",
    "        # ----------SECTION 5----------\n",
    "        # If a neural network baseline is used, set up the targets and the inputs for the \n",
    "        # baseline. \n",
    "        # \n",
    "        # Fit it to the current batch in order to use for the next iteration. Use the \n",
    "        # baseline_update_op you defined earlier.\n",
    "        #\n",
    "        # Hint #bl2: Instead of trying to target raw Q-values directly, rescale the \n",
    "        # targets to have mean zero and std=1. (Goes with Hint #bl1 above.)\n",
    "\n",
    "        # YOUR_CODE_HERE\n",
    "        pass\n",
    "\n",
    "    #====================================================================================#\n",
    "    #                           ----------SECTION 4----------\n",
    "    # Performing the Policy Update\n",
    "    #====================================================================================#\n",
    "\n",
    "    # Call the update operation necessary to perform the policy gradient update based on \n",
    "    # the current batch of rollouts.\n",
    "    # \n",
    "    # For debug purposes, you may wish to save the value of the loss function before\n",
    "    # and after an update, and then log them below. \n",
    "\n",
    "    # YOUR_CODE_HERE\n",
    "    sess.run(update_op, feed_dict={sy_ob_no : ob_no, sy_ac_na : ac_na, sy_adv_n : adv_n})\n",
    "\n",
    "    # Log diagnostics\n",
    "    returns = [path[\"reward\"].sum() for path in paths]\n",
    "    ep_lengths = [pathlength(path) for path in paths]\n",
    "    logz.log_tabular(\"Time\", time.time() - start)\n",
    "    logz.log_tabular(\"Iteration\", itr)\n",
    "    logz.log_tabular(\"AverageReturn\", np.mean(returns))\n",
    "    logz.log_tabular(\"StdReturn\", np.std(returns))\n",
    "    logz.log_tabular(\"MaxReturn\", np.max(returns))\n",
    "    logz.log_tabular(\"MinReturn\", np.min(returns))\n",
    "    logz.log_tabular(\"EpLenMean\", np.mean(ep_lengths))\n",
    "    logz.log_tabular(\"EpLenStd\", np.std(ep_lengths))\n",
    "    logz.log_tabular(\"TimestepsThisBatch\", timesteps_this_batch)\n",
    "    logz.log_tabular(\"TimestepsSoFar\", total_timesteps)\n",
    "    logz.dump_tabular()\n",
    "    logz.pickle_tf_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import logz\n",
    "import scipy.signal\n",
    "import os\n",
    "import time\n",
    "import inspect\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(\n",
    "\t\tinput_placeholder, \n",
    "\t\toutput_size,\n",
    "\t\tscope, \n",
    "\t\tn_layers=2, \n",
    "\t\tsize=64, \n",
    "\t\tactivation=tf.tanh,\n",
    "\t\toutput_activation=None\n",
    "\t\t):\n",
    "\t#========================================================================================#\n",
    "\t#                           ----------SECTION 3----------\n",
    "\t# Network building\n",
    "\t#\n",
    "\t# Your code should make a feedforward neural network (also called a multilayer perceptron)\n",
    "\t# with 'n_layers' hidden layers of size 'size' units. \n",
    "\t# \n",
    "\t# The output layer should have size 'output_size' and activation 'output_activation'.\n",
    "\t#\n",
    "\t# Hint: use tf.layers.dense\n",
    "\t#========================================================================================#\n",
    "\n",
    "\twith tf.variable_scope(scope):\n",
    "\t\t# YOUR_CODE_HERE\n",
    "\t\tout = tf.layers.dense(input_placeholder, size, activation=activation, name=\"fcfirst\")\n",
    "\t\tfor i in range(n_layers):\n",
    "\t\t\tout = tf.layers.dense(out, size, activation=activation, name = \"fc\" + str(i+1))\n",
    "\t\tout = tf.layers.dense(out, output_size, activation=output_activation, name = \"fclast\")\n",
    "\t\treturn out\n",
    "def pathlength(path):\n",
    "\treturn len(path[\"reward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-20 21:39:03,308] Making new env: HalfCheetah-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "6\n",
      "********** Iteration 0 ************\n",
      "----------------------------------------\n",
      "|               Time |            2.11 |\n",
      "|          Iteration |               0 |\n",
      "|      AverageReturn |           -91.4 |\n",
      "|          StdReturn |            36.1 |\n",
      "|          MaxReturn |           -23.1 |\n",
      "|          MinReturn |            -161 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        5.13e+03 |\n",
      "----------------------------------------\n",
      "********** Iteration 1 ************\n",
      "----------------------------------------\n",
      "|               Time |            4.02 |\n",
      "|          Iteration |               1 |\n",
      "|      AverageReturn |           -85.2 |\n",
      "|          StdReturn |            38.5 |\n",
      "|          MaxReturn |             -13 |\n",
      "|          MinReturn |            -165 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.03e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 2 ************\n",
      "----------------------------------------\n",
      "|               Time |            5.95 |\n",
      "|          Iteration |               2 |\n",
      "|      AverageReturn |           -63.3 |\n",
      "|          StdReturn |            30.4 |\n",
      "|          MaxReturn |           -7.96 |\n",
      "|          MinReturn |            -159 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.54e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 3 ************\n",
      "----------------------------------------\n",
      "|               Time |            7.87 |\n",
      "|          Iteration |               3 |\n",
      "|      AverageReturn |           -52.9 |\n",
      "|          StdReturn |            32.2 |\n",
      "|          MaxReturn |            32.1 |\n",
      "|          MinReturn |            -118 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        2.05e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 4 ************\n",
      "----------------------------------------\n",
      "|               Time |            9.78 |\n",
      "|          Iteration |               4 |\n",
      "|      AverageReturn |             -72 |\n",
      "|          StdReturn |            29.7 |\n",
      "|          MaxReturn |           -9.38 |\n",
      "|          MinReturn |            -155 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        2.57e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 5 ************\n",
      "----------------------------------------\n",
      "|               Time |            11.7 |\n",
      "|          Iteration |               5 |\n",
      "|      AverageReturn |           -68.9 |\n",
      "|          StdReturn |            30.8 |\n",
      "|          MaxReturn |            -5.1 |\n",
      "|          MinReturn |            -168 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        3.08e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 6 ************\n",
      "----------------------------------------\n",
      "|               Time |            13.6 |\n",
      "|          Iteration |               6 |\n",
      "|      AverageReturn |           -64.6 |\n",
      "|          StdReturn |            35.9 |\n",
      "|          MaxReturn |           -7.34 |\n",
      "|          MinReturn |            -163 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        3.59e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 7 ************\n",
      "----------------------------------------\n",
      "|               Time |            15.5 |\n",
      "|          Iteration |               7 |\n",
      "|      AverageReturn |           -57.8 |\n",
      "|          StdReturn |            32.9 |\n",
      "|          MaxReturn |            27.3 |\n",
      "|          MinReturn |            -130 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        4.11e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 8 ************\n",
      "----------------------------------------\n",
      "|               Time |            17.4 |\n",
      "|          Iteration |               8 |\n",
      "|      AverageReturn |           -69.9 |\n",
      "|          StdReturn |              39 |\n",
      "|          MaxReturn |            7.68 |\n",
      "|          MinReturn |            -168 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        4.62e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 9 ************\n",
      "----------------------------------------\n",
      "|               Time |            19.4 |\n",
      "|          Iteration |               9 |\n",
      "|      AverageReturn |             -72 |\n",
      "|          StdReturn |            44.3 |\n",
      "|          MaxReturn |             7.6 |\n",
      "|          MinReturn |            -185 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        5.13e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 10 ************\n",
      "----------------------------------------\n",
      "|               Time |            21.3 |\n",
      "|          Iteration |              10 |\n",
      "|      AverageReturn |           -81.6 |\n",
      "|          StdReturn |            30.4 |\n",
      "|          MaxReturn |           -21.1 |\n",
      "|          MinReturn |            -170 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        5.65e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 11 ************\n",
      "----------------------------------------\n",
      "|               Time |            23.2 |\n",
      "|          Iteration |              11 |\n",
      "|      AverageReturn |             -76 |\n",
      "|          StdReturn |            40.1 |\n",
      "|          MaxReturn |             -20 |\n",
      "|          MinReturn |            -252 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        6.16e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 12 ************\n",
      "----------------------------------------\n",
      "|               Time |            25.1 |\n",
      "|          Iteration |              12 |\n",
      "|      AverageReturn |           -73.2 |\n",
      "|          StdReturn |            38.2 |\n",
      "|          MaxReturn |           -22.2 |\n",
      "|          MinReturn |            -166 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        6.67e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 13 ************\n",
      "----------------------------------------\n",
      "|               Time |              27 |\n",
      "|          Iteration |              13 |\n",
      "|      AverageReturn |           -71.6 |\n",
      "|          StdReturn |            34.3 |\n",
      "|          MaxReturn |           -7.11 |\n",
      "|          MinReturn |            -165 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        7.19e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 14 ************\n",
      "----------------------------------------\n",
      "|               Time |              29 |\n",
      "|          Iteration |              14 |\n",
      "|      AverageReturn |           -56.3 |\n",
      "|          StdReturn |              32 |\n",
      "|          MaxReturn |            4.29 |\n",
      "|          MinReturn |            -144 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |         7.7e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 15 ************\n",
      "----------------------------------------\n",
      "|               Time |            30.9 |\n",
      "|          Iteration |              15 |\n",
      "|      AverageReturn |           -68.1 |\n",
      "|          StdReturn |            29.9 |\n",
      "|          MaxReturn |             -24 |\n",
      "|          MinReturn |            -157 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        8.21e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 16 ************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "|               Time |            32.8 |\n",
      "|          Iteration |              16 |\n",
      "|      AverageReturn |             -59 |\n",
      "|          StdReturn |            32.6 |\n",
      "|          MaxReturn |              15 |\n",
      "|          MinReturn |            -127 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        8.73e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 17 ************\n",
      "----------------------------------------\n",
      "|               Time |            34.7 |\n",
      "|          Iteration |              17 |\n",
      "|      AverageReturn |           -56.9 |\n",
      "|          StdReturn |              24 |\n",
      "|          MaxReturn |           -26.4 |\n",
      "|          MinReturn |            -122 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        9.24e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 18 ************\n",
      "----------------------------------------\n",
      "|               Time |            36.6 |\n",
      "|          Iteration |              18 |\n",
      "|      AverageReturn |           -53.8 |\n",
      "|          StdReturn |            33.8 |\n",
      "|          MaxReturn |           -2.53 |\n",
      "|          MinReturn |            -124 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        9.75e+04 |\n",
      "----------------------------------------\n",
      "********** Iteration 19 ************\n",
      "----------------------------------------\n",
      "|               Time |            38.5 |\n",
      "|          Iteration |              19 |\n",
      "|      AverageReturn |           -46.4 |\n",
      "|          StdReturn |            25.9 |\n",
      "|          MaxReturn |           -6.06 |\n",
      "|          MinReturn |            -112 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.03e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 20 ************\n",
      "----------------------------------------\n",
      "|               Time |            40.5 |\n",
      "|          Iteration |              20 |\n",
      "|      AverageReturn |           -47.7 |\n",
      "|          StdReturn |            27.2 |\n",
      "|          MaxReturn |           -8.09 |\n",
      "|          MinReturn |            -104 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.08e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 21 ************\n",
      "----------------------------------------\n",
      "|               Time |            42.4 |\n",
      "|          Iteration |              21 |\n",
      "|      AverageReturn |           -38.5 |\n",
      "|          StdReturn |              19 |\n",
      "|          MaxReturn |            3.65 |\n",
      "|          MinReturn |           -85.5 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.13e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 22 ************\n",
      "----------------------------------------\n",
      "|               Time |            44.3 |\n",
      "|          Iteration |              22 |\n",
      "|      AverageReturn |           -27.8 |\n",
      "|          StdReturn |            24.3 |\n",
      "|          MaxReturn |            20.5 |\n",
      "|          MinReturn |             -88 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.18e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 23 ************\n",
      "----------------------------------------\n",
      "|               Time |            46.2 |\n",
      "|          Iteration |              23 |\n",
      "|      AverageReturn |           -43.6 |\n",
      "|          StdReturn |            36.4 |\n",
      "|          MaxReturn |            12.8 |\n",
      "|          MinReturn |            -161 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.23e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 24 ************\n",
      "----------------------------------------\n",
      "|               Time |            48.2 |\n",
      "|          Iteration |              24 |\n",
      "|      AverageReturn |           -35.2 |\n",
      "|          StdReturn |            25.1 |\n",
      "|          MaxReturn |            26.7 |\n",
      "|          MinReturn |           -97.4 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.28e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 25 ************\n",
      "----------------------------------------\n",
      "|               Time |            50.1 |\n",
      "|          Iteration |              25 |\n",
      "|      AverageReturn |           -38.3 |\n",
      "|          StdReturn |            25.9 |\n",
      "|          MaxReturn |            11.7 |\n",
      "|          MinReturn |            -118 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.33e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 26 ************\n",
      "----------------------------------------\n",
      "|               Time |              52 |\n",
      "|          Iteration |              26 |\n",
      "|      AverageReturn |           -50.6 |\n",
      "|          StdReturn |            43.6 |\n",
      "|          MaxReturn |            30.7 |\n",
      "|          MinReturn |            -129 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.39e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 27 ************\n",
      "----------------------------------------\n",
      "|               Time |            53.9 |\n",
      "|          Iteration |              27 |\n",
      "|      AverageReturn |           -27.7 |\n",
      "|          StdReturn |            26.5 |\n",
      "|          MaxReturn |            29.8 |\n",
      "|          MinReturn |            -106 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.44e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 28 ************\n",
      "----------------------------------------\n",
      "|               Time |            55.8 |\n",
      "|          Iteration |              28 |\n",
      "|      AverageReturn |           -24.8 |\n",
      "|          StdReturn |              47 |\n",
      "|          MaxReturn |            50.5 |\n",
      "|          MinReturn |            -180 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.49e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 29 ************\n",
      "----------------------------------------\n",
      "|               Time |            57.8 |\n",
      "|          Iteration |              29 |\n",
      "|      AverageReturn |             -25 |\n",
      "|          StdReturn |            37.1 |\n",
      "|          MaxReturn |            35.2 |\n",
      "|          MinReturn |            -109 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.54e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 30 ************\n",
      "----------------------------------------\n",
      "|               Time |            59.7 |\n",
      "|          Iteration |              30 |\n",
      "|      AverageReturn |           -17.5 |\n",
      "|          StdReturn |            40.2 |\n",
      "|          MaxReturn |            55.2 |\n",
      "|          MinReturn |            -100 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.59e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 31 ************\n",
      "----------------------------------------\n",
      "|               Time |            61.6 |\n",
      "|          Iteration |              31 |\n",
      "|      AverageReturn |             4.7 |\n",
      "|          StdReturn |            38.5 |\n",
      "|          MaxReturn |            81.2 |\n",
      "|          MinReturn |           -63.2 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.64e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 32 ************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "|               Time |            63.5 |\n",
      "|          Iteration |              32 |\n",
      "|      AverageReturn |          -0.778 |\n",
      "|          StdReturn |            37.3 |\n",
      "|          MaxReturn |            81.9 |\n",
      "|          MinReturn |           -78.3 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.69e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 33 ************\n",
      "----------------------------------------\n",
      "|               Time |            65.4 |\n",
      "|          Iteration |              33 |\n",
      "|      AverageReturn |            1.21 |\n",
      "|          StdReturn |            25.8 |\n",
      "|          MaxReturn |            64.1 |\n",
      "|          MinReturn |           -51.1 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.75e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 34 ************\n",
      "----------------------------------------\n",
      "|               Time |            67.4 |\n",
      "|          Iteration |              34 |\n",
      "|      AverageReturn |            2.73 |\n",
      "|          StdReturn |            29.8 |\n",
      "|          MaxReturn |            76.5 |\n",
      "|          MinReturn |           -59.2 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |         1.8e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 35 ************\n",
      "----------------------------------------\n",
      "|               Time |            69.3 |\n",
      "|          Iteration |              35 |\n",
      "|      AverageReturn |            7.72 |\n",
      "|          StdReturn |            26.9 |\n",
      "|          MaxReturn |            59.2 |\n",
      "|          MinReturn |           -79.4 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.85e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 36 ************\n",
      "----------------------------------------\n",
      "|               Time |            71.2 |\n",
      "|          Iteration |              36 |\n",
      "|      AverageReturn |           -1.59 |\n",
      "|          StdReturn |            18.3 |\n",
      "|          MaxReturn |            28.7 |\n",
      "|          MinReturn |           -50.5 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |         1.9e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 37 ************\n",
      "----------------------------------------\n",
      "|               Time |            73.1 |\n",
      "|          Iteration |              37 |\n",
      "|      AverageReturn |           -14.3 |\n",
      "|          StdReturn |            23.6 |\n",
      "|          MaxReturn |            24.8 |\n",
      "|          MinReturn |           -54.6 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        1.95e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 38 ************\n",
      "----------------------------------------\n",
      "|               Time |            75.1 |\n",
      "|          Iteration |              38 |\n",
      "|      AverageReturn |           -3.49 |\n",
      "|          StdReturn |            28.1 |\n",
      "|          MaxReturn |            50.2 |\n",
      "|          MinReturn |           -65.2 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |           2e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 39 ************\n",
      "----------------------------------------\n",
      "|               Time |              77 |\n",
      "|          Iteration |              39 |\n",
      "|      AverageReturn |           -11.2 |\n",
      "|          StdReturn |            20.2 |\n",
      "|          MaxReturn |            24.1 |\n",
      "|          MinReturn |           -58.8 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        2.05e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 40 ************\n",
      "----------------------------------------\n",
      "|               Time |            78.9 |\n",
      "|          Iteration |              40 |\n",
      "|      AverageReturn |           -11.6 |\n",
      "|          StdReturn |            27.6 |\n",
      "|          MaxReturn |            45.2 |\n",
      "|          MinReturn |           -63.4 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |         2.1e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 41 ************\n",
      "----------------------------------------\n",
      "|               Time |            80.8 |\n",
      "|          Iteration |              41 |\n",
      "|      AverageReturn |           -5.13 |\n",
      "|          StdReturn |            32.8 |\n",
      "|          MaxReturn |            82.7 |\n",
      "|          MinReturn |           -72.4 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        2.16e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 42 ************\n",
      "----------------------------------------\n",
      "|               Time |            82.7 |\n",
      "|          Iteration |              42 |\n",
      "|      AverageReturn |           -9.67 |\n",
      "|          StdReturn |            21.5 |\n",
      "|          MaxReturn |              27 |\n",
      "|          MinReturn |           -55.2 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        2.21e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 43 ************\n",
      "----------------------------------------\n",
      "|               Time |            84.7 |\n",
      "|          Iteration |              43 |\n",
      "|      AverageReturn |           -20.1 |\n",
      "|          StdReturn |            18.9 |\n",
      "|          MaxReturn |            22.1 |\n",
      "|          MinReturn |           -64.4 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        2.26e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 44 ************\n",
      "----------------------------------------\n",
      "|               Time |            86.6 |\n",
      "|          Iteration |              44 |\n",
      "|      AverageReturn |           -12.6 |\n",
      "|          StdReturn |            18.2 |\n",
      "|          MaxReturn |            17.8 |\n",
      "|          MinReturn |           -68.1 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        2.31e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 45 ************\n",
      "----------------------------------------\n",
      "|               Time |            88.5 |\n",
      "|          Iteration |              45 |\n",
      "|      AverageReturn |           -11.4 |\n",
      "|          StdReturn |            12.5 |\n",
      "|          MaxReturn |            12.6 |\n",
      "|          MinReturn |           -39.8 |\n",
      "|          EpLenMean |             151 |\n",
      "|           EpLenStd |               0 |\n",
      "| TimestepsThisBatch |        5.13e+03 |\n",
      "|     TimestepsSoFar |        2.36e+05 |\n",
      "----------------------------------------\n",
      "********** Iteration 46 ************\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "exp_name=''\n",
    "env_name='HalfCheetah-v1'\n",
    "n_iter=100\n",
    "gamma=.9\n",
    "min_timesteps_per_batch=5000\n",
    "max_path_length=150\n",
    "learning_rate=6e-2\n",
    "reward_to_go=True\n",
    "animate=False\n",
    "logdir=None\n",
    "normalize_advantages=True\n",
    "nn_baseline=False\n",
    "seed=64765\n",
    "# network arguments\n",
    "n_layers=4\n",
    "size=128\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Configure output directory for logging\n",
    "\"\"\"\n",
    "logz.configure_output_dir(logdir)\n",
    "\n",
    "# Log experimental parameters\n",
    "args = inspect.getargspec(train_PG)[0]\n",
    "locals_ = locals()\n",
    "params = {k: locals_[k] if k in locals_ else None for k in args}\n",
    "logz.save_params(params)\n",
    "\"\"\"\n",
    "# Set random seeds\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Make the gym environment\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Is this env continuous, or discrete?\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "print(discrete)\n",
    "\n",
    "# Maximum length for episodes\n",
    "max_path_length = max_path_length or env.spec.max_episode_steps\n",
    "\n",
    "#========================================================================================#\n",
    "# Notes on notation:\n",
    "# \n",
    "# Symbolic variables have the prefix sy_, to distinguish them from the numerical values\n",
    "# that are computed later in the function\n",
    "# \n",
    "# Prefixes and suffixes:\n",
    "# ob - observation \n",
    "# ac - action\n",
    "# _no - this tensor should have shape (batch size /n/, observation dim)\n",
    "# _na - this tensor should have shape (batch size /n/, action dim)\n",
    "# _n  - this tensor should have shape (batch size /n/)\n",
    "# \n",
    "# Note: batch size /n/ is defined at runtime, and until then, the shape for that axis\n",
    "# is None\n",
    "#========================================================================================#\n",
    "\n",
    "# Observation and action sizes\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "print(ac_dim)\n",
    "#========================================================================================#\n",
    "#                           ----------SECTION 4----------\n",
    "# Placeholders\n",
    "# \n",
    "# Need these for batch observations / actions / advantages in policy gradient loss function.\n",
    "#========================================================================================#\n",
    "\n",
    "sy_ob_no = tf.placeholder(shape=[None, ob_dim], name=\"ob\", dtype=tf.float32)\n",
    "if discrete:\n",
    "    sy_ac_na = tf.placeholder(shape=[None], name=\"ac\", dtype=tf.int32) \n",
    "else:\n",
    "    sy_ac_na = tf.placeholder(shape=[None, ac_dim], name=\"ac\", dtype=tf.float32) \n",
    "\n",
    "# Define a placeholder for advantages\n",
    "sy_adv_n = tf.placeholder(shape=[None], name=\"adv\", dtype=tf.float32)\n",
    "\n",
    "\n",
    "#========================================================================================#\n",
    "#                           ----------SECTION 4----------\n",
    "# Networks\n",
    "# \n",
    "# Make symbolic operations for\n",
    "#   1. Policy network outputs which describe the policy distribution.\n",
    "#       a. For the discrete case, just logits for each action.\n",
    "#\n",
    "#       b. For the continuous case, the mean / log std of a Gaussian distribution over \n",
    "#          actions.\n",
    "#\n",
    "#      Hint: use the 'build_mlp' function you defined in utilities.\n",
    "#\n",
    "#      Note: these ops should be functions of the placeholder 'sy_ob_no'\n",
    "#\n",
    "#   2. Producing samples stochastically from the policy distribution.\n",
    "#       a. For the discrete case, an op that takes in logits and produces actions.\n",
    "#\n",
    "#          Should have shape [None]\n",
    "#\n",
    "#       b. For the continuous case, use the reparameterization trick:\n",
    "#          The output from a Gaussian distribution with mean 'mu' and std 'sigma' is\n",
    "#\n",
    "#               mu + sigma * z,         z ~ N(0, I)\n",
    "#\n",
    "#          This reduces the problem to just sampling z. (Hint: use tf.random_normal!)\n",
    "#\n",
    "#          Should have shape [None, ac_dim]\n",
    "#\n",
    "#      Note: these ops should be functions of the policy network output ops.\n",
    "#\n",
    "#   3. Computing the log probability of a set of actions that were actually taken, \n",
    "#      according to the policy.\n",
    "#\n",
    "#      Note: these ops should be functions of the placeholder 'sy_ac_na', and the \n",
    "#      policy network output ops.\n",
    "#   \n",
    "#========================================================================================#\n",
    "\n",
    "if discrete:\n",
    "    # YOUR_CODE_HERE\n",
    "    sy_logits_na = build_mlp(sy_ob_no, ac_dim, \"nn\", size=size, n_layers=n_layers)\n",
    "    sy_sampled_ac = tf.reshape(tf.multinomial(tf.nn.log_softmax(sy_logits_na), tf.shape(sy_ob_no)[0]), [tf.shape(sy_ob_no)[0]]) # Hint: Use the tf.multinomial op\n",
    "    sy_logprob_n = tf.nn.softmax_cross_entropy_with_logits(logits = sy_logits_na, labels = tf.one_hot(sy_ac_na, ac_dim))\n",
    "#tf.gather_nd(tf.nn.log_softmax(sy_logits_na), tf.stack([tf.range(tf.shape(sy_logits_na)[0]), sy_ac_na],-1))\n",
    "\n",
    "else:\n",
    "    # YOUR_CODE_HERE\n",
    "    sy_mean = build_mlp(sy_ob_no, ac_dim, \"mean\", size=size, n_layers=n_layers)\n",
    "    sy_logstd = tf.get_variable(name=\"stdev\", shape=[1,  ac_dim]) # logstd should just be a trainable variable, not a network output.\n",
    "    sy_stdev = tf.exp(sy_logstd)+1e-4\n",
    "    sy_var = tf.square(sy_stdev)\n",
    "    sy_sampled_ac = tf.random_normal([tf.shape(sy_ob_no)[0], ac_dim])*sy_stdev + sy_mean\n",
    "    sy_logprob_n = 1/2*(tf.reduce_sum(tf.square(sy_ac_na - sy_mean)/sy_var, axis = 1) + tf.log(tf.reduce_prod(sy_var)))# Hint: Use the log probability under a multivariate gaussian.\n",
    "    \n",
    "#========================================================================================#\n",
    "#                           ----------SECTION 4----------\n",
    "# Loss Function and Training Operation\n",
    "#========================================================================================#\n",
    "\n",
    "loss = tf.reduce_mean(sy_logprob_n*sy_adv_n)# Loss function that we'll differentiate to get the policy gradient.\n",
    "update_op = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True).minimize(loss)\n",
    "\n",
    "\n",
    "#========================================================================================#\n",
    "#                           ----------SECTION 5----------\n",
    "# Optional Baseline\n",
    "#========================================================================================#\n",
    "\n",
    "if nn_baseline:\n",
    "    baseline_prediction = tf.squeeze(build_mlp(\n",
    "                            sy_ob_no, \n",
    "                            1, \n",
    "                            \"nn_baseline\",\n",
    "                            n_layers=n_layers,\n",
    "                            size=size))\n",
    "    # Define placeholders for targets, a loss function and an update op for fitting a \n",
    "    # neural network baseline. These will be used to fit the neural network baseline. \n",
    "    # YOUR_CODE_HERE\n",
    "    target_val = tf.placeholder(shape=[None],name='tv', dtype=tf.float32)\n",
    "    target_rewards = tf.placeholder(shape=[None],name='rw', dtype=tf.float32)\n",
    "    loss = tf.reduce_mean(tf.square(baseline_prediction - target_rewards - gamma*target_val))\n",
    "    \n",
    "    baseline_update_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "#========================================================================================#\n",
    "# Tensorflow Engineering: Config, Session, Variable initialization\n",
    "#========================================================================================#\n",
    "\n",
    "tf_config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1) \n",
    "\n",
    "sess = tf.Session(config=tf_config)\n",
    "sess.__enter__() # equivalent to `with sess:`\n",
    "tf.global_variables_initializer().run() #pylint: disable=E1101\n",
    "\n",
    "\n",
    "\n",
    "#========================================================================================#\n",
    "# Training Loop\n",
    "#========================================================================================#\n",
    "\n",
    "total_timesteps = 0\n",
    "\n",
    "for itr in range(n_iter):\n",
    "    print(\"********** Iteration %i ************\"%itr)\n",
    "\n",
    "    # Collect paths until we have enough timesteps\n",
    "    timesteps_this_batch = 0\n",
    "    paths = []\n",
    "    while True:\n",
    "        ob = env.reset()\n",
    "        obs, acs, rewards = [], [], []\n",
    "        animate_this_episode=(len(paths)==0 and (itr % 10 == 0) and animate)\n",
    "        steps = 0\n",
    "        while True:\n",
    "            if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "            obs.append(ob)\n",
    "            ac = sess.run(sy_sampled_ac, feed_dict={sy_ob_no : ob[None]})\n",
    "            ac = ac[0]\n",
    "            acs.append(ac)\n",
    "            ob, rew, done, _ = env.step(ac)\n",
    "                \n",
    "            rewards.append(rew)\n",
    "            steps += 1\n",
    "            if done or steps > max_path_length:\n",
    "                break\n",
    "        path = {\"observation\" : np.array(obs), \n",
    "                \"reward\" : np.array(rewards), \n",
    "                \"action\" : np.array(acs)}\n",
    "        paths.append(path)\n",
    "        timesteps_this_batch += pathlength(path)\n",
    "        if timesteps_this_batch > min_timesteps_per_batch:\n",
    "            break\n",
    "    total_timesteps += timesteps_this_batch\n",
    "\n",
    "    # Build arrays for observation, action for the policy gradient update by concatenating \n",
    "    # across paths\n",
    "    ob_no = np.concatenate([path[\"observation\"] for path in paths])\n",
    "    ac_na = np.concatenate([path[\"action\"] for path in paths])\n",
    "\n",
    "    #====================================================================================#\n",
    "    #                           ----------SECTION 4----------\n",
    "    # Computing Q-values\n",
    "    #\n",
    "    # Your code should construct numpy arrays for Q-values which will be used to compute\n",
    "    # advantages (which will in turn be fed to the placeholder you defined above). \n",
    "    #\n",
    "    # Recall that the expression for the policy gradient PG is\n",
    "    #\n",
    "    #       PG = E_{tau} [sum_{t=0}^T grad log pi(a_t|s_t) * (Q_t - b_t )]\n",
    "    #\n",
    "    # where \n",
    "    #\n",
    "    #       tau=(s_0, a_0, ...) is a trajectory,\n",
    "    #       Q_t is the Q-value at time t, Q^{pi}(s_t, a_t),\n",
    "    #       and b_t is a baseline which may depend on s_t. \n",
    "    #\n",
    "    # You will write code for two cases, controlled by the flag 'reward_to_go':\n",
    "    #\n",
    "    #   Case 1: trajectory-based PG \n",
    "    #\n",
    "    #       (reward_to_go = False)\n",
    "    #\n",
    "    #       Instead of Q^{pi}(s_t, a_t), we use the total discounted reward summed over \n",
    "    #       entire trajectory (regardless of which time step the Q-value should be for). \n",
    "    #\n",
    "    #       For this case, the policy gradient estimator is\n",
    "    #\n",
    "    #           E_{tau} [sum_{t=0}^T grad log pi(a_t|s_t) * Ret(tau)]\n",
    "    #\n",
    "    #v       where\n",
    "    #\n",
    "    #           Ret(tau) = sum_{t'=0}^T gamma^t' r_{t'}.\n",
    "    #\n",
    "    #       Thus, you should compute\n",
    "    #\n",
    "    #           Q_t = Ret(tau)\n",
    "    #\n",
    "    #   Case 2: reward-to-go PG \n",
    "    #\n",
    "    #       (reward_to_go = True)\n",
    "    #\n",
    "    #       Here, you estimate Q^{pi}(s_t, a_t) by the discounted sum of rewards starting\n",
    "    #       from time step t. Thus, you should compute\n",
    "    #\n",
    "    #           Q_t = sum_{t'=t}^T gamma^(t'-t) * r_{t'}\n",
    "    #\n",
    "    #\n",
    "    # Store the Q-values for all timesteps and all trajectories in a variable 'q_n',\n",
    "    # like the 'ob_no' and 'ac_na' above. \n",
    "    #\n",
    "    #====================================================================================#\n",
    "\n",
    "    # YOUR_CODE_HERE\n",
    "    qs = []\n",
    "    if reward_to_go:\n",
    "        for l in range(len(paths)):\n",
    "            rewards = paths[l]['reward']\n",
    "            n_actions = rewards.shape[0]\n",
    "            q = np.zeros((n_actions))\n",
    "            for i in range(n_actions):\n",
    "                for j in range(n_actions-i):\n",
    "                    q[i] += rewards[i + j]*np.power(gamma, j)\n",
    "            qs.append(q)\n",
    "    else:\n",
    "        for l in range(len(paths)):\n",
    "            rewards = paths[l]['reward']\n",
    "            n_actions = rewards.shape[0]\n",
    "            q = np.zeros((n_actions))\n",
    "            for i in range(n_actions):\n",
    "                q += rewards[i]*np.power(gamma, i)\n",
    "            qs.append(q)\n",
    "    q_n = np.concatenate(qs)\n",
    "    #====================================================================================#\n",
    "    #                           ----------SECTION 5----------\n",
    "    # Computing Baselines\n",
    "    #====================================================================================#\n",
    "\n",
    "    if nn_baseline:\n",
    "        # If nn_baseline is True, use your neural network to predict reward-to-go\n",
    "        # at each timestep for each trajectory, and save the result in a variable 'b_n'\n",
    "        # like 'ob_no', 'ac_na', and 'q_n'.\n",
    "        #\n",
    "        # Hint #bl1: rescale the output from the nn_baseline to match the statistics\n",
    "        # (mean and std) of the current or previous batch of Q-values. (Goes with Hint\n",
    "        # #bl2 below.)\n",
    "\n",
    "        b_n = sess.run(baseline_prediction, feed_dict={sy_ob_no : ob_no})\n",
    "        b_n = (b_n - np.mean(b_n))/np.std(b_n)*np.std(q_n) + np.mean(q_n)\n",
    "        adv_n = q_n - b_n\n",
    "    else:\n",
    "        adv_n = q_n.copy()\n",
    "\n",
    "    #====================================================================================#\n",
    "    #                           ----------SECTION 4----------\n",
    "    # Advantage Normalization\n",
    "    #====================================================================================#\n",
    "\n",
    "    if normalize_advantages:\n",
    "        # On the next line, implement a trick which is known empirically to reduce variance\n",
    "        # in policy gradient methods: normalize adv_n to have mean zero and std=1. \n",
    "        # YOUR_CODE_HERE\n",
    "        mean = np.mean(adv_n)\n",
    "        stdev = np.std(adv_n)\n",
    "        adv_n = (adv_n - mean)/stdev\n",
    "\n",
    "\n",
    "    #====================================================================================#\n",
    "    #                           ----------SECTION 5----------\n",
    "    # Optimizing Neural Network Baseline\n",
    "    #====================================================================================#\n",
    "    if nn_baseline:\n",
    "        # ----------SECTION 5----------\n",
    "        # If a neural network baseline is used, set up the targets and the inputs for the \n",
    "        # baseline. \n",
    "        # \n",
    "        # Fit it to the current batch in order to use for the next iteration. Use the \n",
    "        # baseline_update_op you defined earlier.\n",
    "        #\n",
    "        # Hint #bl2: Instead of trying to target raw Q-values directly, rescale the \n",
    "        # targets to have mean zero and std=1. (Goes with Hint #bl1 above.)\n",
    "\n",
    "        # YOUR_CODE_HERE\n",
    "        targets_base = gamma * b_n[1:]\n",
    "        rewards_base = np.concatenate([path[\"reward\"] for path in paths])[:-1]\n",
    "        obs_base = ob_no[:-1, :]\n",
    "        \n",
    "        sess.run(baseline_update_op, feed_dict = {sy_ob_no: obs_base, target_val: targets_base, target_rewards: rewards_base})\n",
    "\n",
    "    #====================================================================================#\n",
    "    #                           ----------SECTION 4----------\n",
    "    # Performing the Policy Update\n",
    "    #====================================================================================#\n",
    "\n",
    "    # Call the update operation necessary to perform the policy gradient update based on \n",
    "    # the current batch of rollouts.\n",
    "    # \n",
    "    # For debug purposes, you may wish to save the value of the loss function before\n",
    "    # and after an update, and then log them below. \n",
    "\n",
    "    # YOUR_CODE_HERE\n",
    "    sess.run(update_op, feed_dict={sy_ob_no : ob_no, sy_ac_na : ac_na, sy_adv_n : adv_n})\n",
    "    # Log diagnostics\n",
    "    returns = [path[\"reward\"].sum() for path in paths]\n",
    "    ep_lengths = [pathlength(path) for path in paths]\n",
    "    logz.log_tabular(\"Time\", time.time() - start)\n",
    "    logz.log_tabular(\"Iteration\", itr)\n",
    "    logz.log_tabular(\"AverageReturn\", np.mean(returns))\n",
    "    logz.log_tabular(\"StdReturn\", np.std(returns))\n",
    "    logz.log_tabular(\"MaxReturn\", np.max(returns))\n",
    "    logz.log_tabular(\"MinReturn\", np.min(returns))\n",
    "    logz.log_tabular(\"EpLenMean\", np.mean(ep_lengths))\n",
    "    logz.log_tabular(\"EpLenStd\", np.std(ep_lengths))\n",
    "    logz.log_tabular(\"TimestepsThisBatch\", timesteps_this_batch)\n",
    "    logz.log_tabular(\"TimestepsSoFar\", total_timesteps)\n",
    "    logz.dump_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e1e23f5baa4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 6. 8.]\n"
     ]
    }
   ],
   "source": [
    "inp = tf.placeholder(dtype=tf.float32)\n",
    "ind = tf.placeholder(dtype=tf.int32)\n",
    "\n",
    "indexes = tf.stack([tf.range(tf.shape(inp)[0]), ind],-1)\n",
    "out = tf.gather_nd(inp, indexes)#tf.stack([tf.range(tf.shape(inp)[0]), ind],-1))\n",
    "\n",
    "sess = tf.Session()\n",
    "i = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "index = np.array([1,2,1], dtype = np.int32)\n",
    "o = sess.run(out, feed_dict = {inp : i, ind : index})\n",
    "print(o)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "out = tf.multinomial((np.softmaprob), 100)\n",
    "sess = tf.Session()\n",
    "\n",
    "p = np.array([[-5000000000, 1000000000]])\n",
    "o = sess.run(out, feed_dict = {prob : p})\n",
    "print(o)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
